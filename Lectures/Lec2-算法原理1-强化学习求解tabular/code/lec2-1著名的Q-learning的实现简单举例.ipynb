{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第 1 章 初探强化学习\n",
    "\n",
    "## 1.1 简介\n",
    "\n",
    "初探强化学习是想帮助理解强化学习的算法原理，提高代码实践能力，更能了解是否开始研究决策智能这个方向。\n",
    "\n",
    "在机器学习领域，有一类重要的任务和人生选择很相似，即序贯决策（sequential decision making）任务。决策和预测任务不同，决策<font color=red>**往往会带来“后果”**</font>，因此决策者需要为未来负责，在未来的时间点做出进一步的决策。实现序贯决策的机器学习方法就是强化学习（reinforcement learning）。预测仅仅产生一个针对输入数据的信号，并期望它和未来可观测到的信号一致，这不会使未来情况发生任何改变。\n",
    "\n",
    "本章主要讨论强化学习的基本概念和思维方式；包括了解强化学习在解决什么任务，其基本的数学刻画是什么样的，学习的目标是什么，以及它和预测型的有监督学习方法有什么根本性的区别。\n",
    "\n",
    "## 1.2 什么是强化学习\n",
    "\n",
    "广泛地讲，强化学习是机器通过与环境交互来实现目标的一种计算方法。机器和环境的一轮交互是指，机器在环境的一个状态下做一个动作决策，把这个动作作用到环境当中，环境发生相应的改变，并且将相应的奖励反馈和下一轮的状态传回机器。这种交互是迭代进行的，机器的目标是最大化在多轮交互过程中获得的**累积奖励的期望**。强化学习用智能体（agent）这个概念来表示做决策的机器。\n",
    "\n",
    "> 相比于有监督学习中的“模型”，强化学习中的“智能体”强调机器不但可以感知周围的环境信息，还可以通过做决策来直接改变这个环境，而不只是给出一些预测信号。\n",
    "\n",
    "智能体和环境之间具体的交互方式如图1-1所示。在每一轮交互中，智能体感知到环境目前所处的状态，经过自身的计算给出本轮的动作，将其作用到环境中；环境得到智能体的动作后，产生相应的即时奖励信号并发生相应的状态转移。智能体则在下一轮交互中感知到新的环境状态，依次类推。\n",
    "\n",
    "<img src=\"https://hrl.boyuai.com/static/11.da5ee18f.png\" alt=\"img\" style=\"zoom:50%;\" />\n",
    "\n",
    "​                                                                           图1-1 强化学习中智能体和环境之间的迭代式交互\n",
    "\n",
    "智能体有3种关键要素，即**感知**、**决策**和**奖励**。\n",
    "\n",
    "- **感知**。智能体在某种程度上感知环境的状态，从而知道自己所处的现状。例如，下围棋的智能体感知当前的棋盘情况；无人车感知周围道路的车辆、行人和红绿灯等情况；机器狗通过摄像头感知面前的图像，通过脚底的力学传感器来感知地面的摩擦功率和倾斜度等情况。\n",
    "- **决策。**智能体根据当前的状态计算出达到目标需要采取的动作的过程叫作决策。例如，针对当前的棋盘决定下一颗落子的位置；针对当前的路况，无人车计算出方向盘的角度和刹车、油门的力度；针对当前收集到的视觉和力觉信号，机器狗给出4条腿的齿轮的角速度。策略是智能体最终体现出的智能形式，是不同智能体之间的核心区别。\n",
    "- **奖励。**环境根据状态和智能体采取的动作，产生一个标量信号作为奖励反馈。这个标量信号衡量智能体这一轮动作的好坏。例如，围棋博弈是否胜利；无人车是否安全、平稳且快速地行驶；机器狗是否在前进而没有摔倒。最大化累积奖励期望是智能体提升策略的目标，也是衡量智能体策略好坏的关键指标。\n",
    "\n",
    "从以上分析可以看出，面向决策任务的强化学习和面向预测任务的有监督学习在形式上是有不少区别的。\n",
    "\n",
    "- 首先，决策任务往往涉及多轮交互，即序贯决策；而预测任务总是单轮的独立任务。如果决策也是单轮的，那么它可以转化为“判别最优动作”的预测任务。\n",
    "- 其次，因为决策任务是多轮的，智能体就需要在每轮做决策时考虑未来环境相应的改变，所以当前轮带来最大奖励反馈的动作，在长期来看并不一定是最优的。\n",
    "\n",
    "## 1.3 强化学习的环境\n",
    "\n",
    "我们从1.2节可以看到，强化学习的智能体是在和一个动态环境的交互中完成序贯决策的。我们说一个环境是动态的，意思就是它会随着某些因素的变化而不断演变，这在数学和物理中往往用随机过程来刻画。其实，生活中几乎所有的系统都在进行演变，例如一座城市的交通、一片湖中的生态、一场足球比赛、一个星系等。对于一个随机过程，其最关键的要素就**是状态以及状态转移的条件概率分布**。这就好比一个微粒在水中的布朗运动可以由它的起始位置以及下一刻的位置相对当前位置的条件概率分布来刻画。\n",
    "\n",
    "如果在环境这样一个自身演变的随机过程中加入一个外来的干扰因素，即智能体的动作，那么环境的下一刻状态的概率分布将由当前状态和智能体的动作来共同决定，用最简单的数学公式表示则是\n",
    "$$\n",
    "下一状态 ~ P(.|当前状态,智能体的动作)\n",
    "$$\n",
    "根据上式可知，智能体决策的动作作用到环境中，使得环境发生相应的状态改变，而智能体接下来则需要在新的状态下进一步给出决策。\n",
    "\n",
    "> 小结：与面向决策任务的智能体进行交互的环境是一个**动态的随机过程**，其未来状态的分布由当前状态和智能体决策的动作来共同决定，并且每一轮状态转移都伴随着两方面的随机性：\n",
    ">\n",
    "> 一是智能体决策的动作的随机性;\n",
    ">\n",
    "> 二是环境基于当前状态和智能体动作来采样下一刻状态的随机性。\n",
    ">\n",
    "> 通过对环境的动态随机过程的刻画，可以看出，在**动态随机过程中学习**和在一个固定的数据分布下学习是非常不同的。\n",
    "\n",
    "## 1.4 强化学习的目标\n",
    "\n",
    "在上述动态环境下，智能体和环境每次进行交互时，环境会产生相应的奖励信号，其往往由实数标量来表示。这个奖励信号一般是诠释当前状态或动作的好坏的及时反馈信号，好比在玩游戏的过程中某一个操作获得的分数值。整个交互过程的每一轮获得的奖励信号可以进行累加，形成智能体的整体回报（return），好比一盘游戏最后的分数值。根据环境的动态性我们可以知道，即使环境和智能体策略不变，智能体的初始状态也不变，智能体和环境交互产生的结果也很可能是不同的，对应获得的回报也会不同。因此，在强化学习中，我们关注回报的期望，并将其定义为价值（value），这就是强化学习中智能体学习的优化目标。\n",
    "\n",
    "价值的计算有些复杂，因为需要对交互过程中每一轮智能体采取动作的概率分布和环境相应的状态转移的概率分布做积分运算。强化学习和有监督学习的**学习目标其实是一致的**，即在某个数据分布下优化一个价值(scalar)的期望。不过，经过后面的分析我们会发现，强化学习和有监督学习的**优化途径是不同的**。\n",
    "\n",
    "\n",
    "## 1.5 强化学习中的数据\n",
    "\n",
    "本节是从数据层面谈谈有监督学习和强化学习的区别。\n",
    "\n",
    "有监督学习的任务建立在从给定的数据分布中采样得到的训练数据集上，通过优化在训练数据集中设定的目标函数（如最小化预测误差）来找到模型的最优参数。这里，训练数据集背后的数据分布是完全不变的。\n",
    "\n",
    "在强化学习中，数据是在智能体与环境交互的过程中得到的。如果智能体不采取某个决策动作，那么该动作对应的数据就永远无法被观测到，所以当前智能体的训练数据来自之前智能体的决策结果。因此，智能体的策略不同，与环境交互所产生的数据分布就不同，如图1-2所示。\n",
    "\n",
    "<img src=\"https://hrl.boyuai.com/static/12.683654d1.png\" alt=\"img\" style=\"zoom:50%;\" />\n",
    "\n",
    "<center>图1-2 强化学习中智能体与环境交互产生相应的数据分布</center >\n",
    "\n",
    "具体而言，强化学习中有一个关于数据分布的概念，叫作**占用度量（occupancy measure）**，其具体的数学定义和性质会在第3章讨论，在这里先简要陈述下：归一化的占用度量用于衡量在一个智能体决策与一个动态环境的交互过程中，采样到一个具体的状态动作对（state-action pair）的概率分布。\n",
    "\n",
    "占用度量有一个很重要的性质：给定两个策略及其与一个动态环境交互得到的两个占用度量，那么当且仅当这两个占用度量相同时，这两个策略相同。也就是说，如果一个智能体的策略有所改变，那么它和环境交互得到的占用度量也会相应改变。\n",
    "\n",
    "根据占用度量这一重要的性质，我们可以领悟到强化学习本质的思维方式。\n",
    "\n",
    "- 强化学习的策略在训练中会不断更新，其对应的数据分布（即占用度量）也会相应地改变。因此，强化学习的一大难点就在于，智能体看到的数据分布是随着智能体的学习而不断发生改变的。\n",
    "- 由于奖励建立在状态动作对之上，一个策略对应的价值其实就是一个占用度量下对应的奖励的期望，因此**寻找最优策略对应着寻找最优占用度量。**\n",
    "- 训练的过程中 智能体与环境交互数据集 -> <font color=\"red\">策略提升 </font>  ->  决策动作 -> 收集数据 -> 智能体与环境交互数据集\n",
    "    - 策略提升是个比较的概念。要想实现策略提升，首先定义策略价值；而要想定义策略价值，先需要对问题进行建模，再将策略价值数值化。问题建模 -> 定义策略价值 -> 策略提升方法\n",
    "\n",
    "## 1.6 强化学习的独特性\n",
    "\n",
    "对强化学习的基本数学概念有了一定的了解后，再看看一般的有监督学习和强化学习的区别。\n",
    "\n",
    "对于一般的有监督学习任务，我们的目标是找到一个最优的模型函数，使其在训练数据集上最小化一个给定的损失函数。在训练数据独立同分布的假设下，这个优化目标表示最小化模型在整个数据分布上的**泛化误差**（generalization error），用简要的公式可以概括为：\n",
    "\n",
    "$$\n",
    "{ 最优模型 = \\underset {模型} { \\operatorname {arg\\,min} } \\, \\mathbb{E}_{(特征，标签)\\sim 数据分布} \\quad \\quad \\quad ([损失函数(标签，模型(特征))]. }\n",
    "$$\n",
    "相比之下，强化学习任务的最终优化目标是**最大化智能体策略在和动态环境交互过程中的价值**。根据1.5节的分析，策略的价值可以等价转换成奖励函数在策略的占用度量上的期望，即：\n",
    "$$\n",
    " { 最优策略 = \\underset {策略} { \\operatorname {arg\\,min} } \\, \\mathbb{E}_{(状态，动作)\\sim 策略的占用度量} \\quad \\quad \\quad \\quad ([奖励函数(状态，动作\t)]. }\n",
    "$$\n",
    "观察以上两个优化公式，回顾1.4节，总结出两者的相似点和不同点。\n",
    "\n",
    "- 有监督学习和强化学习的**优化目标相似**，即都是在优化某个数据分布下的一个分数值的期望。\n",
    "- 二者**优化的途径是不同的**，有监督学习直接通过优化模型对于数据特征的输出来优化目标，即修改目标函数而数据分布不变；强化学习则通过改变策略来调整智能体和环境交互数据的分布，进而优化目标，即修改数据分布而目标函数不变。\n",
    "\n",
    "综上所述，一般有监督学习和强化学习的范式之间的区别为：\n",
    "\n",
    "- 一般的有监督学习关注寻找一个模型，使其在给定数据分布下得到的损失函数的期望最小；\n",
    "- 强化学习关注寻找一个智能体策略，使其在与动态环境交互的过程中产生最优的数据分布，即最大化该分布下一个给定奖励函数的期望。\n",
    "\n",
    "强化学习不同于机器学习的方面：\n",
    "1. 强化学习没有supervisor，只有一个reward信号\n",
    "2. 反馈/奖励是延迟的，not instantaneous\n",
    "3. 时序决策，non i.i.d 数据\n",
    "4. agent的动作影响agent后续接收到的数据:观察 + reward\n",
    "\n",
    "\n",
    "## 1.7 强化学习的中重要的概念\n",
    "\n",
    "### rewards\n",
    "\n",
    "### \n",
    "## 1.8 强化学习的历史\n",
    "\n",
    "### AlfaGohe AlfaZero\n",
    "强化学习的历史记住了3个瞬间\n",
    "- 李世石在还剩6分多钟的时候的一手棋，此后人类对AlfaGo再无胜绩；\n",
    "- 柯洁输棋后的眼泪，以及说出的下棋过程中的绝望，虽然仍然不相信机器智能会超过人类，但是这段话还是很有触动的；\n",
    "- 2017年AlfaZero横空出世后，横扫AlfaGo 100：0。\n",
    "\n",
    "也记住了两个决定\n",
    "- AlfaZero抛弃了Haman Knowledge，在围棋界传承了上百年的哲学也迅速产生了很大的改变。\n",
    "- David在围棋的强化学习领域10年磨一剑的决定。\n",
    "\n",
    "### 强化学习的理论\n",
    "补充下理论的发展 标志性的进步是Q-learning -> DQ-Learning\n",
    "\n",
    "以下给出一个基于gridworld建立的agent-environment交互的例子，理解如何建立环境，agent，以及tabular表示的$Q^*$值。\n",
    "\n",
    "1. 创建一个在环境中随机选择动作的agent。 \n",
    "2. 创建一个使用 Q-learning 的agent,初始 Q 值为0、ε-贪婪策略函数的随机参数 ε=0.05，学习率 α=0.1。随意尝试设置这三个参数为其他值。 \n",
    "3. 绘制两个agent通过episode获得的平均总奖励。这称为学习曲线。t运行足够多的episode，以使得采用Q 学习的agen收敛到接近最优的策略。\n",
    "\n",
    "#### agent-environment接口\n",
    "agent与其环境的交互从决策阶段 t=0 开始，观察当前状态 s0。 （请注意，在这个初始阶段没有奖励。）然后代理在决策阶段 t=1 选择要执行的动作。环境通过将其状态更改为 s1 并返回数字奖励信号 r1 来响应。\n",
    "\n",
    "![The agent-environment interface](images/agent-environment.png)\n",
    "\n",
    "#### 环境：gridworld中的导航\n",
    "\n",
    "agent在每个state（方格）有四种可能的行动：西部、北部、南部和东部。动作不靠谱。他们以 0.8 的概率将智能体移动到预期的方向，并以 0.2 的概率将智能体移动到随机的其他方向。如果移动方向受阻，则agent保持在相同的方格中。agent的初始状态是底部的五个网格正方形之一，随机选择。带有金色和炸弹的网格方块是终端状态。如果代理发现自己处于这些方格之一，则episode结束。然后一个新的episode开始于处于初始状态的agent。 使用强化学习算法来计算最佳策略，以尽可能少的步骤找到黄金，同时避免炸弹。为此，我们将使用以下奖励函数：每次导航操作为-1，找到金币额外+10，击中炸弹额外-10。例如，转移到带有金色的正方形的即时奖励 是-1 + 10 = + 9。 不要使用折扣（即设置 γ=1）。\n",
    "\n",
    "![gridworld中的导航](images/gold.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_reward = np.array([[0.02, 0.03, 0.04, 0], [0.01, 0, 0.02, 0], [0, 0.01, 0.02, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOARD_ROWS = 3\n",
    "BOARD_COLS = 4\n",
    "WIN_STATE = (0, 3)\n",
    "LOSE_STATE = (1, 3)\n",
    "START = (2, 0)\n",
    "DETERMINISTIC = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State:\n",
    "    def __init__(self, state=START):\n",
    "        self.board = np.zeros([BOARD_ROWS, BOARD_COLS])\n",
    "        self.board[1, 1] = -1\n",
    "        self.state = state\n",
    "        self.isEnd = False\n",
    "        self.determine = DETERMINISTIC\n",
    "        \n",
    "    def giveReward(self):\n",
    "        if self.state == WIN_STATE:\n",
    "            return 1\n",
    "        elif self.state == LOSE_STATE:\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def isEndFunc(self):\n",
    "        if (self.state == WIN_STATE) or (self.state == LOSE_STATE):\n",
    "            self.isEnd = True\n",
    "    \n",
    "    def nxtPosition(self, action):\n",
    "        \"\"\"\n",
    "        action: up, down, left, right\n",
    "        -------------\n",
    "        0 | 1 | 2| 3|\n",
    "        1 |\n",
    "        2 |\n",
    "        return next position\n",
    "        \"\"\"\n",
    "        if self.determine:\n",
    "            if action == \"up\":\n",
    "                nxtState = (self.state[0]-1, self.state[1])\n",
    "            elif action == \"down\":\n",
    "                nxtState = (self.state[0]+1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                nxtState = (self.state[0], self.state[1]-1)\n",
    "            else:\n",
    "                nxtState = (self.state[0], self.state[1]+1)\n",
    "            # if next state legal\n",
    "            if (nxtState[0] >= 0) and (nxtState[0] <= 2):\n",
    "                if (nxtState[1] >= 0) and (nxtState[1] <= 3):\n",
    "                    if nxtState != (1, 1):\n",
    "                        return nxtState\n",
    "            return self.state\n",
    "    \n",
    "    def showBoard(self):\n",
    "        self.board[self.state] = 1\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('-----------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                if self.board[i, j] == 1:\n",
    "                    token = '*'\n",
    "                if self.board[i, j] == -1:\n",
    "                    token = 'z'\n",
    "                if self.board[i, j] == 0:\n",
    "                    token = '0'\n",
    "                out += token + ' | '\n",
    "            print(out)\n",
    "        print('-----------------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = State()\n",
    "s.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.states = []\n",
    "        self.actions = [\"up\", \"down\", \"left\", \"right\"]\n",
    "        self.State = State()\n",
    "        self.isEnd = self.State.isEnd\n",
    "        self.lr = 0.2\n",
    "        self.exp_rate = 0.3\n",
    "        \n",
    "        # initial state reward\n",
    "        self.state_values = {}\n",
    "        for i in range(BOARD_ROWS):\n",
    "            for j in range(BOARD_COLS):\n",
    "                self.state_values[(i, j)] = 0  # init_reward[i, j]\n",
    "    \n",
    "    def chooseAction(self):\n",
    "        # choose action with most expected value\n",
    "        mx_nxt_reward = 0\n",
    "        action = \"\"\n",
    "        \n",
    "        if np.random.uniform(0, 1) <= self.exp_rate:\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            # greedy action\n",
    "            for a in self.actions:\n",
    "                # if the action is deterministic\n",
    "                nxt_reward = self.state_values[self.State.nxtPosition(a)]\n",
    "                if nxt_reward >= mx_nxt_reward:\n",
    "                    action = a\n",
    "                    mx_nxt_reward = nxt_reward\n",
    "            # print(\"current pos: {}, greedy aciton: {}\".format(self.State.state, action))\n",
    "        return action\n",
    "    \n",
    "    def takeAction(self, action):\n",
    "        position = self.State.nxtPosition(action)\n",
    "        return State(state=position)     \n",
    "    \n",
    "    def reset(self):\n",
    "        self.states = []\n",
    "        self.State = State()\n",
    "        self.isEnd = self.State.isEnd\n",
    "    \n",
    "    def play(self, rounds=10):\n",
    "        i = 0\n",
    "        while i < rounds:\n",
    "            # to the end of game back propagate reward\n",
    "            if self.State.isEnd:\n",
    "                # back propagate\n",
    "                reward = self.State.giveReward()\n",
    "                # explicitly assign end state to reward values\n",
    "                self.state_values[self.State.state] = reward\n",
    "                print(\"Game End Reward\", reward)\n",
    "                for s in reversed(self.states):\n",
    "                    reward = self.state_values[s] + self.lr*(reward - self.state_values[s])\n",
    "                    self.state_values[s] = round(reward, 3)\n",
    "                self.reset()\n",
    "                i += 1\n",
    "            else:\n",
    "                action = self.chooseAction()\n",
    "                # append trace\n",
    "                self.states.append(self.State.nxtPosition(action))\n",
    "                print(\"current position {} action {}\".format(self.State.state, action))\n",
    "                # by taking the action, it reaches the next state\n",
    "                self.State = self.takeAction(action)\n",
    "                # mark is end\n",
    "                self.State.isEndFunc()\n",
    "                print(\"nxt state\", self.State.state)\n",
    "                print(\"---------------------\")\n",
    "                self.isEnd = self.State.isEnd\n",
    "    \n",
    "    def showValues(self):\n",
    "        for i in range(0, BOARD_ROWS):\n",
    "            print('----------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(0, BOARD_COLS):\n",
    "                out += str(self.state_values[(i, j)]) + ' | '\n",
    "            print(out)\n",
    "        print('----------------------------------') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action right\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action right\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action right\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action right\n",
      "nxt state (2, 3)\n",
      "---------------------\n",
      "current position (2, 3) action up\n",
      "nxt state (1, 3)\n",
      "---------------------\n",
      "Game End Reward -1\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action down\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action down\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action down\n",
      "nxt state (2, 2)\n",
      "---------------------\n",
      "current position (2, 2) action up\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action down\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action right\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action down\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action right\n",
      "nxt state (2, 1)\n",
      "---------------------\n",
      "current position (2, 1) action left\n",
      "nxt state (2, 0)\n",
      "---------------------\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action down\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action left\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action down\n",
      "nxt state (1, 2)\n",
      "---------------------\n",
      "current position (1, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n",
      "current position (2, 0) action up\n",
      "nxt state (1, 0)\n",
      "---------------------\n",
      "current position (1, 0) action up\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action left\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action up\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action left\n",
      "nxt state (0, 0)\n",
      "---------------------\n",
      "current position (0, 0) action right\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action down\n",
      "nxt state (0, 1)\n",
      "---------------------\n",
      "current position (0, 1) action right\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action up\n",
      "nxt state (0, 2)\n",
      "---------------------\n",
      "current position (0, 2) action right\n",
      "nxt state (0, 3)\n",
      "---------------------\n",
      "Game End Reward 1\n"
     ]
    }
   ],
   "source": [
    "ag = Agent()\n",
    "\n",
    "ag.play(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| 0.935 | 0.933 | 0.92 | 1.0 | \n",
      "----------------------------------\n",
      "| 0.889 | 0 | 0.524 | -1.0 | \n",
      "----------------------------------\n",
      "| 0.563 | 0.226 | 0.033 | -0.2 | \n",
      "----------------------------------\n"
     ]
    }
   ],
   "source": [
    "ag.showValues()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-learning\n",
    "From Sutton & Barto\n",
    "![Q-learning](images/q-learning.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this exercise, you will implement the interaction of a reinforecment learning agent with its environment. \n",
    "# We will use the gridworld environment https://github.com/michaeltinsley/Gridworld-with-Q-Learning-Reinforcement-Learning-/blob/master/Gridworld.ipynb\n",
    "import numpy as np\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes for the Enviroment and the Agent\n",
    "\n",
    "- GridWorld 类包含环境 \n",
    "- 定义了环境的维度 \n",
    "- 存储所有奖励的位置 \n",
    "- 编写不同方法的函数 \n",
    "    - get_available_actions 返回可能的操作 \n",
    "    - agent_on_map 打印出代理在网格上的当前位置（用于调试） \n",
    "    - get_reward 返回输入位置的奖励 \n",
    "    - make_step 沿指定方向移动代理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    ## Initialise starting data\n",
    "    def __init__(self):\n",
    "        # Set information about the gridworld\n",
    "        self.height = 5\n",
    "        self.width = 5\n",
    "        self.grid = np.zeros(( self.height, self.width)) - 1\n",
    "        \n",
    "        # Set random start location for the agent\n",
    "        self.current_location = ( 4, np.random.randint(0,5))\n",
    "        \n",
    "        # Set locations for the bomb and the gold\n",
    "        self.bomb_location = (1,3)\n",
    "        self.gold_location = (0,3)\n",
    "        self.terminal_states = [ self.bomb_location, self.gold_location]\n",
    "        \n",
    "        # Set grid rewards for special cells\n",
    "        self.grid[ self.bomb_location[0], self.bomb_location[1]] = -10\n",
    "        self.grid[ self.gold_location[0], self.gold_location[1]] = 10\n",
    "        \n",
    "        # Set available actions\n",
    "        self.actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "    \n",
    "        \n",
    "    ## Put methods here:\n",
    "    def get_available_actions(self):\n",
    "        \"\"\"Returns possible actions\"\"\"\n",
    "        return self.actions\n",
    "    \n",
    "    def agent_on_map(self):\n",
    "        \"\"\"Prints out current location of the agent on the grid (used for debugging)\"\"\"\n",
    "        grid = np.zeros(( self.height, self.width))\n",
    "        grid[ self.current_location[0], self.current_location[1]] = 1\n",
    "        return grid\n",
    "    \n",
    "    def get_reward(self, new_location):\n",
    "        \"\"\"Returns the reward for an input position\"\"\"\n",
    "        return self.grid[ new_location[0], new_location[1]]\n",
    "        \n",
    "    \n",
    "    def make_step(self, action):\n",
    "        \"\"\"Moves the agent in the specified direction. If agent is at a border, agent stays still\n",
    "        but takes negative reward. Function returns the reward for the move.\"\"\"\n",
    "        # Store previous location\n",
    "        last_location = self.current_location\n",
    "        \n",
    "        # UP\n",
    "        if action == 'UP':\n",
    "            # If agent is at the top, stay still, collect reward\n",
    "            if last_location[0] == 0:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0] - 1, self.current_location[1])\n",
    "                reward = self.get_reward(self.current_location)\n",
    "        \n",
    "        # DOWN\n",
    "        elif action == 'DOWN':\n",
    "            # If agent is at bottom, stay still, collect reward\n",
    "            if last_location[0] == self.height - 1:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0] + 1, self.current_location[1])\n",
    "                reward = self.get_reward(self.current_location)\n",
    "            \n",
    "        # LEFT\n",
    "        elif action == 'LEFT':\n",
    "            # If agent is at the left, stay still, collect reward\n",
    "            if last_location[1] == 0:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0], self.current_location[1] - 1)\n",
    "                reward = self.get_reward(self.current_location)\n",
    "\n",
    "        # RIGHT\n",
    "        elif action == 'RIGHT':\n",
    "            # If agent is at the right, stay still, collect reward\n",
    "            if last_location[1] == self.width - 1:\n",
    "                reward = self.get_reward(last_location)\n",
    "            else:\n",
    "                self.current_location = ( self.current_location[0], self.current_location[1] + 1)\n",
    "                reward = self.get_reward(self.current_location)\n",
    "                \n",
    "        return reward\n",
    "    \n",
    "    def check_state(self):\n",
    "        \"\"\"Check if the agent is in a terminal state (gold or bomb), if so return 'TERMINAL'\"\"\"\n",
    "        if self.current_location in self.terminal_states:\n",
    "            return 'TERMINAL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent():        \n",
    "    # Choose a random action\n",
    "    def choose_action(self, available_actions):\n",
    "        \"\"\"Returns a random choice of the available actions\"\"\"\n",
    "        return np.random.choice(available_actions)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_Agent():\n",
    "    # Intialise\n",
    "    def __init__(self, environment, epsilon=0.05, alpha=0.1, gamma=1):\n",
    "        self.environment = environment\n",
    "        self.q_table = dict() # Store all Q-values in dictionary of dictionaries \n",
    "        for x in range(environment.height): # Loop through all possible grid spaces, create sub-dictionary for each\n",
    "            for y in range(environment.width):\n",
    "                self.q_table[(x,y)] = {'UP':0, 'DOWN':0, 'LEFT':0, 'RIGHT':0} # Populate sub-dictionary with zero values for possible moves\n",
    "\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def choose_action(self, available_actions):\n",
    "        \"\"\"Returns the optimal action from Q-Value table. If multiple optimal actions, chooses random choice.\n",
    "        Will make an exploratory random action dependent on epsilon.\"\"\"\n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            action = available_actions[np.random.randint(0, len(available_actions))]\n",
    "        else:\n",
    "            q_values_of_state = self.q_table[self.environment.current_location]\n",
    "            maxValue = max(q_values_of_state.values())\n",
    "            action = np.random.choice([k for k, v in q_values_of_state.items() if v == maxValue])\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def learn(self, old_state, reward, new_state, action):\n",
    "        \"\"\"Updates the Q-value table using Q-learning\"\"\"\n",
    "        q_values_of_state = self.q_table[new_state]\n",
    "        max_q_value_in_new_state = max(q_values_of_state.values())\n",
    "        current_q_value = self.q_table[old_state][action]\n",
    "        \n",
    "        self.q_table[old_state][action] = (1 - self.alpha) * current_q_value + self.alpha * (reward + self.gamma * max_q_value_in_new_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(environment, agent, trials=500, max_steps_per_episode=1000, learn=False):\n",
    "    \"\"\"The play function runs iterations and updates Q-values if desired.\"\"\"\n",
    "    reward_per_episode = [] # Initialise performance log\n",
    "    \n",
    "    for trial in range(trials): # Run trials\n",
    "        cumulative_reward = 0 # Initialise values of each game\n",
    "        step = 0\n",
    "        game_over = False\n",
    "        while step < max_steps_per_episode and game_over != True: # Run until max steps or until game is finished\n",
    "            old_state = environment.current_location\n",
    "            action = agent.choose_action(environment.actions) \n",
    "            reward = environment.make_step(action)\n",
    "            new_state = environment.current_location\n",
    "            \n",
    "            if learn == True: # Update Q-values if learning is specified\n",
    "                agent.learn(old_state, reward, new_state, action)\n",
    "                \n",
    "            cumulative_reward += reward\n",
    "            step += 1\n",
    "            \n",
    "            if environment.check_state() == 'TERMINAL': # If game is in terminal state, game over and start next trial\n",
    "                environment.__init__()\n",
    "                game_over = True     \n",
    "                \n",
    "        reward_per_episode.append(cumulative_reward) # Append reward for current trial to performance log\n",
    "        \n",
    "    return reward_per_episode # Return performance log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run Random Agent\n",
    "\n",
    "- 随机代理随机移动，不会从它的动作中学习。 \n",
    "- 随机代理是为 Q-Learning 代理提供了基础性能比较\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current position of the agent = (4, 3)\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "Available_actions = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
      "Randomly chosen action = DOWN\n",
      "Reward obtained = -1.0\n",
      "Current position of the agent = (4, 3)\n",
      "[[0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "env = GridWorld()\n",
    "agent = RandomAgent()\n",
    "\n",
    "print(\"Current position of the agent =\", env.current_location)\n",
    "print(env.agent_on_map())\n",
    "available_actions = env.get_available_actions()\n",
    "print(\"Available_actions =\", available_actions)\n",
    "chosen_action = agent.choose_action(available_actions)\n",
    "print(\"Randomly chosen action =\", chosen_action)\n",
    "reward = env.make_step(chosen_action)\n",
    "print(\"Reward obtained =\", reward)\n",
    "print(\"Current position of the agent =\", env.current_location)\n",
    "print(env.agent_on_map())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here the random agent is ran for 500 trials\n",
    "- Performance is obviously inconsistent and not optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e355dfbe50>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize environment and agent\n",
    "environment = GridWorld()\n",
    "random_agent = RandomAgent()\n",
    "\n",
    "reward_per_episode = play(environment, random_agent, trials=500)\n",
    "\n",
    "# Simple learning curve\n",
    "plt.plot(reward_per_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Agent\n",
    "\n",
    "- Here the Q-Learning agent is ran for 500 trials again\n",
    "- Performance is plotted\n",
    "- Performance increases greatly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e3565874f0>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "environment = GridWorld()\n",
    "agentQ = Q_Agent(environment)\n",
    "\n",
    "# Note the learn=True argument!\n",
    "reward_per_episode = play(environment, agentQ, trials=500, learn=True)\n",
    "\n",
    "# Simple learning curve\n",
    "plt.plot(reward_per_episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the final Q-value table with nice formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n",
      "\tUP\n",
      "\t\t-0.30000000000000004\n",
      "\tDOWN\n",
      "\t\t-0.31891\n",
      "\tLEFT\n",
      "\t\t-0.30000000000000004\n",
      "\tRIGHT\n",
      "\t\t-0.12409556392245474\n",
      "(0, 1)\n",
      "\tUP\n",
      "\t\t-0.2\n",
      "\tDOWN\n",
      "\t\t-0.1\n",
      "\tLEFT\n",
      "\t\t-0.1\n",
      "\tRIGHT\n",
      "\t\t5.708594107080454\n",
      "(0, 2)\n",
      "\tUP\n",
      "\t\t5.490232320171997\n",
      "\tDOWN\n",
      "\t\t0.7999998832820968\n",
      "\tLEFT\n",
      "\t\t0.707576468240101\n",
      "\tRIGHT\n",
      "\t\t9.999999999999995\n",
      "(0, 3)\n",
      "\tUP\n",
      "\t\t0\n",
      "\tDOWN\n",
      "\t\t0\n",
      "\tLEFT\n",
      "\t\t0\n",
      "\tRIGHT\n",
      "\t\t0\n",
      "(0, 4)\n",
      "\tUP\n",
      "\t\t-0.1\n",
      "\tDOWN\n",
      "\t\t-0.1\n",
      "\tLEFT\n",
      "\t\t6.5132155990000005\n",
      "\tRIGHT\n",
      "\t\t-0.1\n",
      "(1, 0)\n",
      "\tUP\n",
      "\t\t-0.521949\n",
      "\tDOWN\n",
      "\t\t-0.5894694879423121\n",
      "\tLEFT\n",
      "\t\t-0.5000000000000001\n",
      "\tRIGHT\n",
      "\t\t-0.5433955352751162\n",
      "(1, 1)\n",
      "\tUP\n",
      "\t\t0.8554217926205563\n",
      "\tDOWN\n",
      "\t\t-0.1892996902355513\n",
      "\tLEFT\n",
      "\t\t-0.30000000000000004\n",
      "\tRIGHT\n",
      "\t\t-0.281\n",
      "(1, 2)\n",
      "\tUP\n",
      "\t\t8.999999999999982\n",
      "\tDOWN\n",
      "\t\t3.539944932676038\n",
      "\tLEFT\n",
      "\t\t-0.24526676908008643\n",
      "\tRIGHT\n",
      "\t\t-6.12579511\n",
      "(1, 3)\n",
      "\tUP\n",
      "\t\t0\n",
      "\tDOWN\n",
      "\t\t0\n",
      "\tLEFT\n",
      "\t\t0\n",
      "\tRIGHT\n",
      "\t\t0\n",
      "(1, 4)\n",
      "\tUP\n",
      "\t\t1.9528213050900005\n",
      "\tDOWN\n",
      "\t\t-0.21900000000000003\n",
      "\tLEFT\n",
      "\t\t-1.0\n",
      "\tRIGHT\n",
      "\t\t-0.2\n",
      "(2, 0)\n",
      "\tUP\n",
      "\t\t-0.9076135278900003\n",
      "\tDOWN\n",
      "\t\t-0.9350410277504624\n",
      "\tLEFT\n",
      "\t\t-0.9000000000000004\n",
      "\tRIGHT\n",
      "\t\t-0.6457304074060486\n",
      "(2, 1)\n",
      "\tUP\n",
      "\t\t-0.6670096439609\n",
      "\tDOWN\n",
      "\t\t-0.6717475680300001\n",
      "\tLEFT\n",
      "\t\t-0.7212120200000002\n",
      "\tRIGHT\n",
      "\t\t3.1837971270617587\n",
      "(2, 2)\n",
      "\tUP\n",
      "\t\t7.999999999999978\n",
      "\tDOWN\n",
      "\t\t2.097642992305471\n",
      "\tLEFT\n",
      "\t\t-0.2880959603183209\n",
      "\tRIGHT\n",
      "\t\t-0.3419191011360498\n",
      "(2, 3)\n",
      "\tUP\n",
      "\t\t-1.0\n",
      "\tDOWN\n",
      "\t\t-0.6680204766900001\n",
      "\tLEFT\n",
      "\t\t2.4987298839109835\n",
      "\tRIGHT\n",
      "\t\t-0.7685431229900002\n",
      "(2, 4)\n",
      "\tUP\n",
      "\t\t-0.34654827296709995\n",
      "\tDOWN\n",
      "\t\t-0.8174804704589103\n",
      "\tLEFT\n",
      "\t\t-0.6088209739000001\n",
      "\tRIGHT\n",
      "\t\t-0.6900000000000002\n",
      "(3, 0)\n",
      "\tUP\n",
      "\t\t-1.3723818754371722\n",
      "\tDOWN\n",
      "\t\t-1.4664347072348485\n",
      "\tLEFT\n",
      "\t\t-1.4000000000000008\n",
      "\tRIGHT\n",
      "\t\t-0.9966653305908942\n",
      "(3, 1)\n",
      "\tUP\n",
      "\t\t-1.0584050680475376\n",
      "\tDOWN\n",
      "\t\t-1.1812245386374947\n",
      "\tLEFT\n",
      "\t\t-1.1070221750433074\n",
      "\tRIGHT\n",
      "\t\t3.386461915281876\n",
      "(3, 2)\n",
      "\tUP\n",
      "\t\t6.9999999999999725\n",
      "\tDOWN\n",
      "\t\t0.9317373856217801\n",
      "\tLEFT\n",
      "\t\t-0.04203989905790553\n",
      "\tRIGHT\n",
      "\t\t-0.49619277062277173\n",
      "(3, 3)\n",
      "\tUP\n",
      "\t\t-1.0396517207121272\n",
      "\tDOWN\n",
      "\t\t-1.1107906525795146\n",
      "\tLEFT\n",
      "\t\t3.0027244621256823\n",
      "\tRIGHT\n",
      "\t\t-1.058807676441\n",
      "(3, 4)\n",
      "\tUP\n",
      "\t\t-1.1831220371091542\n",
      "\tDOWN\n",
      "\t\t-1.2745421631892706\n",
      "\tLEFT\n",
      "\t\t-1.209906925379566\n",
      "\tRIGHT\n",
      "\t\t-1.2000000000000006\n",
      "(4, 0)\n",
      "\tUP\n",
      "\t\t-1.963943499013277\n",
      "\tDOWN\n",
      "\t\t-1.4111721013429515\n",
      "\tLEFT\n",
      "\t\t-1.41951457097703\n",
      "\tRIGHT\n",
      "\t\t3.9987132635968137\n",
      "(4, 1)\n",
      "\tUP\n",
      "\t\t-1.5576817966819194\n",
      "\tDOWN\n",
      "\t\t-1.0400231148849934\n",
      "\tLEFT\n",
      "\t\t-0.5149196193692323\n",
      "\tRIGHT\n",
      "\t\t4.999999626158666\n",
      "(4, 2)\n",
      "\tUP\n",
      "\t\t5.999999999999967\n",
      "\tDOWN\n",
      "\t\t1.5841653289528037\n",
      "\tLEFT\n",
      "\t\t0.8117158106420694\n",
      "\tRIGHT\n",
      "\t\t0.36844900867500296\n",
      "(4, 3)\n",
      "\tUP\n",
      "\t\t-0.9606077592014354\n",
      "\tDOWN\n",
      "\t\t-1.49514335802841\n",
      "\tLEFT\n",
      "\t\t4.999999067233562\n",
      "\tRIGHT\n",
      "\t\t-1.0547386762177287\n",
      "(4, 4)\n",
      "\tUP\n",
      "\t\t-1.7279997789574069\n",
      "\tDOWN\n",
      "\t\t-0.5048969789126727\n",
      "\tLEFT\n",
      "\t\t3.997958091179519\n",
      "\tRIGHT\n",
      "\t\t-1.3204266520800656\n"
     ]
    }
   ],
   "source": [
    "def pretty(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('\\t' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            pretty(value, indent+1)\n",
    "        else:\n",
    "            print('\\t' * (indent+1) + str(value))\n",
    "\n",
    "\n",
    "pretty(agentQ.q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- \n",
    "## 1.9 小结\n",
    "\n",
    "本章通过简短的篇幅，大致介绍了强化学习的样貌，梳理了强化学习和有监督学习在范式以及思维方式上的相似点和不同点。在大多数情况下，强化学习任务往往比一般的有监督学习任务更难，因为一旦策略有所改变，其交互产生的数据分布也会随之改变，并且这样的改变是高度复杂、不可追踪的，往往不能用显式的数学公式刻画。这就好像一个混沌系统，我们无法得到其中一个初始设置对应的最终状态分布，而一般的有监督学习任务并没有这样的混沌效应。\n",
    "\n",
    "接下来，就要通过理论学习和代码实践来躬身入局强化学习的大门了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "第2章-多臂老虎机问题.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
